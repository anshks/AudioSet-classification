{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd9e34cd-6309-4ebd-9ecc-50f63219c26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEATs and BEATsConfig imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import importlib.util\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# Add the BEATs module path\n",
    "sys.path.append(\"/scratch/rd3629/ml-project/AudioSet-classification/beats\")\n",
    "\n",
    "# Define the path to the BEATs.py file\n",
    "BEATs_path = \"/scratch/rd3629/ml-project/AudioSet-classification/beats/BEATs.py\"\n",
    "\n",
    "# Load the BEATs module\n",
    "spec = importlib.util.spec_from_file_location(\"BEATs\", BEATs_path)\n",
    "BEATs_module = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"BEATs\"] = BEATs_module\n",
    "spec.loader.exec_module(BEATs_module)\n",
    "\n",
    "# Import classes from the module\n",
    "BEATs = BEATs_module.BEATs\n",
    "BEATsConfig = BEATs_module.BEATsConfig\n",
    "\n",
    "print(\"BEATs and BEATsConfig imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "752e3e99-7372-40d4-88a4-88abe627d818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54139371/ipykernel_2728384/803150140.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('/scratch/rd3629/ml-project/AudioSet-classification/Pre-trained-model/BEATs_iter3_plus_AS2M.pt')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the checkpoint and initialize the model\n",
    "checkpoint = torch.load('/scratch/rd3629/ml-project/AudioSet-classification/Pre-trained-model/BEATs_iter3_plus_AS2M.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2ebef26-0510-4e1c-899c-bc87c436890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BEATs(\n",
       "  (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "  (patch_embedding): Conv2d(1, 512, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "  (dropout_input): Dropout(p=0.1, inplace=False)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (pos_conv): Sequential(\n",
       "      (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "      (1): SamePad()\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (relative_attention_bias): Embedding(320, 12)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1-11): 11 x TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "          (relative_attention_bias): Embedding(320, 12)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = BEATsConfig(checkpoint['cfg'])\n",
    "BEATs_model = BEATs(cfg)\n",
    "BEATs_model.load_state_dict(checkpoint['model'])\n",
    "BEATs_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94e32f00-9cdf-467f-ac41-cf3e7c97ba2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54139371/ipykernel_2728384/2257568278.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  audio_input = torch.load('/scratch/rd3629/ml-project/AudioSet-classification/Data/train/resampled_waveforms.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20550, 160172])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure the model is on GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BEATs_model.to(device)\n",
    "\n",
    "# Load audio input and ensure it is on the correct device\n",
    "audio_input = torch.load('/scratch/rd3629/ml-project/AudioSet-classification/Data/train/resampled_waveforms.pt')\n",
    "audio_input = audio_input.squeeze().to(device)\n",
    "print(audio_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22a41f2f-acdd-43bd-88ad-1320def3e597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:02, 22.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 765.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 3, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 741.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 4, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 725.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 5, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 733.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 6, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 735.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 7, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 756.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 8, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 724.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 9, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 743.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 10, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 728.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 11, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 739.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 12, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 744.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 13, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 716.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 14, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 747.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 15, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 722.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 16, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 740.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 17, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 718.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 18, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 750.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 19, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 748.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 20, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 743.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 21, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 754.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 22, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 724.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 23, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 747.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 24, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 741.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 25, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 723.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 26, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 737.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 27, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 718.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 28, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 758.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 29, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 703.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 30, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 731.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 31, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 722.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 32, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 734.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 33, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 729.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 34, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 708.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 35, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 722.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 36, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 734.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 37, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 750.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 38, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 740.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 39, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 739.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 40, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 721.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 41, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 722.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 42, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 739.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 43, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 738.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 44, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 736.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 45, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 751.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 46, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 746.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 47, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 758.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 48, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 717.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 49, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 737.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 50, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 721.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 51, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 731.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 52, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 742.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 53, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 719.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 54, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 720.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 55, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 732.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 56, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 726.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 57, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 729.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 58, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 714.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 59, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 742.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 60, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 719.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 61, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 743.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 62, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 685.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 63, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 727.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 64, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 656.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 65, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 740.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 66, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 741.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 67, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 716.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 68, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 756.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 69, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 752.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 70, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 727.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 71, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 735.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 72, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 734.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 73, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 750.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 74, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 735.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 75, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 732.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 76, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 721.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 77, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 740.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 78, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 726.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 79, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 721.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 80, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 732.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 81, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 714.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 82, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 737.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 83, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 739.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 84, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 749.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 85, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 729.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 86, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 729.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 87, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 723.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 88, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 744.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 89, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 719.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 90, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 727.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 91, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 747.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 92, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 730.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 93, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 742.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 94, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 728.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 95, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 722.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 96, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 695.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 97, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 727.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 98, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 744.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 99, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 715.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 100, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 737.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 101, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 733.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 102, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 738.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 103, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 711.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 104, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 733.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 105, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 744.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 106, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 734.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 107, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 657.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 108, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 713.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 109, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 724.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 110, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 744.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 111, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 720.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 112, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 730.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 113, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 745.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 114, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 737.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 115, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 737.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 116, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 747.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 117, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 740.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 118, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 737.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 119, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 748.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 120, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 733.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 121, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 731.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 122, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 720.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 123, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 745.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 124, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 746.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 125, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 726.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 126, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 698.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 127, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 736.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 128, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 714.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 129, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 745.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 130, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 728.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 131, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 750.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 132, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 652.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 133, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 704.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 134, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 747.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 135, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 728.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 136, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 712.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 137, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 713.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 138, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 734.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 139, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 725.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 140, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 731.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 141, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 663.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 142, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 725.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 143, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 715.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 144, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 751.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 145, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 739.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 146, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 715.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 147, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 724.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 148, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 722.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 149, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 708.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 150, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 725.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 151, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 731.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 152, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 720.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 153, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 717.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 154, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 721.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 155, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 729.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 156, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 693.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 157, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 746.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 158, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 738.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 159, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 719.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 160, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 741.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 161, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 722.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 162, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 729.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 163, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 739.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 164, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 676.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 165, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 728.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 166, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 728.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 167, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 735.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 168, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 710.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 169, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 741.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 170, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 692.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 171, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 743.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 172, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 698.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 173, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 724.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 174, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 747.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 175, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 721.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 176, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 737.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 177, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 741.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 178, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 667.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 179, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 735.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 180, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 743.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 181, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 730.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 182, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 712.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 183, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 747.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 184, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 757.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 185, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 703.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 186, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 717.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 187, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 765.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 188, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 711.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 189, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 739.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 190, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 726.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 191, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 708.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 192, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 721.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 193, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 656.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 194, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 716.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 195, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 706.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 196, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 674.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 197, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 737.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 198, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 709.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 199, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 683.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 200, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 731.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 201, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 734.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 202, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 746.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 203, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 739.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 204, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 722.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 205, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 691.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 206, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 761.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 207, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 740.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 208, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 730.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 209, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 751.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 210, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 741.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 211, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 738.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 212, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 749.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 213, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 661.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 214, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 704.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 215, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 738.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 216, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 715.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 217, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 705.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 218, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 750.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 219, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 731.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 220, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 743.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 221, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 736.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 222, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 725.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 223, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 747.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 224, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 728.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 225, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 718.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 226, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 735.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 227, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 709.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 228, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 752.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 229, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 721.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 230, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 707.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 231, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 735.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 232, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 728.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 233, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 761.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 234, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 752.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 235, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 732.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 236, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 725.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 237, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 748.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 238, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 738.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 239, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 726.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 240, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 743.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 241, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 749.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 242, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 743.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 243, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 712.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 244, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 734.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 245, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 772.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 246, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 723.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 247, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 712.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 248, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 708.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 249, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 754.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 250, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 670.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 251, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 721.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 252, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 732.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 253, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 708.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 254, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 743.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 255, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 727.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 256, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 698.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 257, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 748.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 258, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 712.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 259, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 732.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 260, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 746.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 261, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 739.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 262, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 736.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 263, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 705.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 264, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 665.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 265, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 729.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 266, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 733.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 267, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 741.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 268, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 725.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 269, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 736.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 270, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 749.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 271, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 737.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 272, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 704.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 273, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 748.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 274, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 718.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 275, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 713.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 276, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 747.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 277, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 713.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 278, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 754.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 279, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 740.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 280, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 705.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 281, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 745.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 282, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 730.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 283, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 736.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 284, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 658.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 285, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 748.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 286, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 702.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 287, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 712.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 288, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 705.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 289, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 713.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 290, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 707.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 291, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 733.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 292, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 719.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 293, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 739.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 294, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 743.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 295, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 760.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 296, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 733.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 297, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 726.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 298, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 713.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 299, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 687.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 300, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 723.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 301, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 726.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 302, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 729.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 303, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 742.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 304, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 729.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 305, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 724.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 306, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 731.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 307, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 720.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 308, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 739.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 309, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 733.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 310, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 737.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 311, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 749.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 312, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 757.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 313, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 745.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 314, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 705.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 315, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 745.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 316, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 757.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 317, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 733.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 318, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 741.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 319, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 742.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 320, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 704.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 321, shape: torch.Size([64, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 719.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 322, shape: torch.Size([6, 160172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 600.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total representation shape: torch.Size([20550, 496, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the batch size\n",
    "batch_size = 64  # Reduce batch size to prevent memory issues\n",
    "\n",
    "# Get the number of samples\n",
    "num_samples = audio_input.size(0)\n",
    "representations = []\n",
    "\n",
    "# Disable gradient calculations\n",
    "with torch.no_grad():\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch = audio_input[i:i + batch_size]  # Slice the tensor for the current batch\n",
    "        print(f\"Processing batch {i // batch_size + 1}, shape: {batch.shape}\")\n",
    "\n",
    "        # Extract features\n",
    "        representation = BEATs_model.extract_features(batch)[-1].detach().cpu()\n",
    "        representations.append(representation)\n",
    "\n",
    "        # Free memory\n",
    "        del batch, representation\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# Concatenate all representations\n",
    "total_representation = torch.cat(representations, dim=0)\n",
    "\n",
    "print(\"Total representation shape:\", total_representation.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ae1a07-a07b-4e79-8e62-d889b0072204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save representation to file\n",
    "torch.save(total_representation, 'train_representation.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
