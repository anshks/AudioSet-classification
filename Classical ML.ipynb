{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24423320-a0cc-4135-ae22-07bf68fa5d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54616500/ipykernel_1561551/4218358896.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load('/scratch/rd3629/ml-project/AudioSet-classification/Data/train_rep.pt')\n",
      "/state/partition1/job-54616500/ipykernel_1561551/4218358896.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load('/scratch/rd3629/ml-project/AudioSet-classification/Data/test_rep.pt')\n",
      "/state/partition1/job-54616500/ipykernel_1561551/4218358896.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_labels = torch.load('/scratch/rd3629/ml-project/AudioSet-classification/Data/train/labels.pt')\n",
      "/state/partition1/job-54616500/ipykernel_1561551/4218358896.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_labels = torch.load('/scratch/rd3629/ml-project/AudioSet-classification/Data/test/labels.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 32.16 seconds.\n",
      "Applying max pooling along the time dimension (496)...\n",
      "(20550, 768)\n",
      "Max pooled train data shape: (20550, 768)\n",
      "Max pooled test data shape: (18886, 768)\n",
      "Max pooling completed in 2.96 seconds.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import average_precision_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Function to calculate MAP\n",
    "def calculate_map(targets, predictions):\n",
    "    print(\"Calculating Mean Average Precision (MAP)...\")\n",
    "    map_score = average_precision_score(targets, predictions, average=\"macro\")\n",
    "    print(f\"MAP calculation complete: {map_score:.4f}\")\n",
    "    return map_score\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "start_time = time.time()\n",
    "train_data = torch.load('/scratch/rd3629/ml-project/AudioSet-classification/Data/train_rep.pt')\n",
    "test_data = torch.load('/scratch/rd3629/ml-project/AudioSet-classification/Data/test_rep.pt')\n",
    "train_labels = torch.load('/scratch/rd3629/ml-project/AudioSet-classification/Data/train/labels.pt')\n",
    "test_labels = torch.load('/scratch/rd3629/ml-project/AudioSet-classification/Data/test/labels.pt')\n",
    "print(f\"Data loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# Apply max pooling along the time dimension (496)\n",
    "print(\"Applying max pooling along the time dimension (496)...\")\n",
    "start_time = time.time()\n",
    "train_data = torch.mean(train_data, dim=1)  # Max pooling across dimension 1 (time)\n",
    "test_data = torch.mean(test_data, dim=1)    # Max pooling across dimension 1 (time)\n",
    "\n",
    "train_data = train_data.numpy()\n",
    "test_data = test_data.numpy()\n",
    "# train_data = train_data.reshape(train_data.shape[0], -1)\n",
    "# test_data = test_data.reshape(test_data.shape[0], -1)\n",
    "# train_data = torch.max(train_data, dim=1).values.numpy()  # Max pooling across dimension 1 (time)\n",
    "# test_data = torch.max(test_data, dim=1).values.numpy()    # Max pooling across dimension 1 (time)\n",
    "print(train_data.shape)\n",
    "train_labels = train_labels.numpy()  # [20550, 527]\n",
    "test_labels = test_labels.numpy()    # [18886, 527]\n",
    "print(f\"Max pooled train data shape: {train_data.shape}\")  # [20550, 768]\n",
    "print(f\"Max pooled test data shape: {test_data.shape}\")    # [18886, 768]\n",
    "print(f\"Max pooling completed in {time.time() - start_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77c2584f-a768-447d-a123-75336128f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved PCA model and transformed data...\n",
      "PCA model and transformed data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import average_precision_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import joblib\n",
    "import time\n",
    "# Dimensionality reduction using PCA\n",
    "pca_path = \"/scratch/rd3629/ml-project/AudioSet-classification/Data/pca_model_100.pkl\"\n",
    "transformed_data_path = \"/scratch/rd3629/ml-project/AudioSet-classification/Data/transformed_data_100.npz\"\n",
    "\n",
    "try:\n",
    "    # Load saved PCA model and transformed data\n",
    "    print(\"Loading saved PCA model and transformed data...\")\n",
    "    pca = joblib.load(pca_path)\n",
    "    with np.load(transformed_data_path) as data:\n",
    "        train_data = data['train']\n",
    "        test_data = data['test']\n",
    "    print(\"PCA model and transformed data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Saved PCA model not found. Applying PCA...\")\n",
    "    pca = PCA(n_components=100)\n",
    "    train_data = pca.fit_transform(train_data)\n",
    "    test_data = pca.transform(test_data)\n",
    "    print(f\"Dimensionality reduced to 512 dimensions. Train shape: {train_data.shape}, Test shape: {test_data.shape}\")\n",
    "\n",
    "    # Save PCA model and transformed data\n",
    "    joblib.dump(pca, pca_path)\n",
    "    np.savez(transformed_data_path, train=train_data, test=test_data)\n",
    "    print(\"PCA model and transformed data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae7abf06-f851-4082-b2e4-63b775d80eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Decision Tree model with OneVsRestClassifier...\n",
      "Training Decision Tree model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m OneVsRestClassifier(\n\u001b[1;32m      5\u001b[0m     DecisionTreeClassifier(\n\u001b[1;32m      6\u001b[0m         criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Use 'entropy' if you prefer information gain\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Decision Tree model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_data, train_labels)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training complete in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Predict probabilities on the test set\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/sklearn/multiclass.py:370\u001b[0m, in \u001b[0;36mOneVsRestClassifier.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    366\u001b[0m columns \u001b[38;5;241m=\u001b[39m (col\u001b[38;5;241m.\u001b[39mtoarray()\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m Y\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# In cases where individual estimators are very fast to train setting\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# n_jobs > 1 in can results in slower performance due to the overhead\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# of spawning threads.  See joblib issue #112.\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)(\n\u001b[1;32m    371\u001b[0m     delayed(_fit_binary)(\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator,\n\u001b[1;32m    373\u001b[0m         X,\n\u001b[1;32m    374\u001b[0m         column,\n\u001b[1;32m    375\u001b[0m         fit_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[1;32m    376\u001b[0m         classes\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    377\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_binarizer_\u001b[38;5;241m.\u001b[39mclasses_[i],\n\u001b[1;32m    378\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_binarizer_\u001b[38;5;241m.\u001b[39mclasses_[i],\n\u001b[1;32m    379\u001b[0m         ],\n\u001b[1;32m    380\u001b[0m     )\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(columns)\n\u001b[1;32m    382\u001b[0m )\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_features_in_\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:1703\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_abort()\n\u001b[1;32m   1704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1706\u001b[0m     \u001b[38;5;66;03m# Store the unconsumed tasks and terminate the workers if necessary\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:1614\u001b[0m, in \u001b[0;36mParallel._abort\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborted \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabort_everything\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m   1610\u001b[0m     \u001b[38;5;66;03m# If the backend is managed externally we need to make sure\u001b[39;00m\n\u001b[1;32m   1611\u001b[0m     \u001b[38;5;66;03m# to leave it in a working state to allow for future jobs\u001b[39;00m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;66;03m# scheduling.\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m     ensure_ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_managed_backend\n\u001b[0;32m-> 1614\u001b[0m     backend\u001b[38;5;241m.\u001b[39mabort_everything(ensure_ready\u001b[38;5;241m=\u001b[39mensure_ready)\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/joblib/_parallel_backends.py:620\u001b[0m, in \u001b[0;36mLokyBackend.abort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabort_everything\u001b[39m(\u001b[38;5;28mself\u001b[39m, ensure_ready\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    618\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Shutdown the workers and restart a new one with the same parameters\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mterminate(kill_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ensure_ready:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/joblib/executor.py:75\u001b[0m, in \u001b[0;36mMemmappingExecutor.terminate\u001b[0;34m(self, kill_workers)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mterminate\u001b[39m(\u001b[38;5;28mself\u001b[39m, kill_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshutdown(kill_workers\u001b[38;5;241m=\u001b[39mkill_workers)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# When workers are killed in a brutal manner, they cannot execute the\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# finalizer of their shared memmaps. The refcount of those memmaps may\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# be off by an unknown number, so instead of decref'ing them, we force\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# with allow_non_empty=True but if we can't, it will be clean up later\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# on by the resource_tracker.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit_resize_lock:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:1287\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m   1281\u001b[0m         partial(_process_chunk, fn),\n\u001b[1;32m   1282\u001b[0m         _get_chunks(chunksize, \u001b[38;5;241m*\u001b[39miterables),\n\u001b[1;32m   1283\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1284\u001b[0m     )\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chain_from_iterable_of_lists(results)\n\u001b[0;32m-> 1287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshutdown\u001b[39m(\u001b[38;5;28mself\u001b[39m, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, kill_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1288\u001b[0m     mp\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutting down executor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flags\u001b[38;5;241m.\u001b[39mflag_as_shutting_down(kill_workers)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Train a Decision Tree Model with OneVsRestClassifier\n",
    "print(\"Initializing Decision Tree model with OneVsRestClassifier...\")\n",
    "start_time = time.time()\n",
    "model = OneVsRestClassifier(\n",
    "    DecisionTreeClassifier(\n",
    "        criterion='gini',  # Use 'entropy' if you prefer information gain\n",
    "        max_depth=5,    # Adjust if you want to limit tree depth\n",
    "        random_state=42    # For reproducibility\n",
    "    ),\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"Training Decision Tree model...\")\n",
    "model.fit(train_data, train_labels)\n",
    "print(f\"Model training complete in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "print(\"Generating predictions on test data...\")\n",
    "start_time = time.time()\n",
    "test_predictions = model.predict_proba(test_data)  # [18886, 527]\n",
    "print(f\"Predictions generated in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# Metrics\n",
    "print(\"Calculating evaluation metrics...\")\n",
    "start_time = time.time()\n",
    "test_map = calculate_map(test_labels, test_predictions)\n",
    "test_f1 = f1_score(test_labels, (test_predictions > 0.5).astype(float), average=\"micro\")\n",
    "print(f\"F1-Score calculation complete: {test_f1:.4f}\")\n",
    "print(f\"Metrics calculated in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Test MAP: {test_map:.4f}\")\n",
    "print(f\"Test F1-Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfaa4b41-47a3-4408-975d-b64397f08920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Decision Tree model with OneVsRestClassifier...\n",
      "Training Decision Tree model...\n",
      "Model training complete in 3.72 seconds.\n",
      "Generating predictions on test data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "\n",
    "# Train a Decision Tree Model with OneVsRestClassifier\n",
    "print(\"Initializing Decision Tree model with OneVsRestClassifier...\")\n",
    "start_time = time.time()\n",
    "# model = OneVsRestClassifier(\n",
    "#     DecisionTreeClassifier(\n",
    "#         criterion='gini',  # Use 'entropy' if you prefer information gain\n",
    "#         max_depth=5,    # Adjust if you want to limit tree depth\n",
    "#         random_state=42    # For reproducibility\n",
    "#     ),\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# model = MultiOutputClassifier(\n",
    "#     RandomForestClassifier(\n",
    "#         n_estimators=100,      # Number of trees in the forest\n",
    "#         criterion='gini',      # Use 'entropy' if you prefer information gain\n",
    "#         max_depth=5,        # None means nodes are expanded until all leaves are pure or contain less than min_samples_split samples\n",
    "#         random_state=42,       # For reproducibility\n",
    "#         n_jobs=-1              # Use all processors for parallelization\n",
    "#     ),\n",
    "#     n_jobs=-1  # Parallelize OneVsRestClassifier as well\n",
    "# )\n",
    "\n",
    "# model = MultiOutputClassifier(\n",
    "#     ExtraTreeClassifier(\n",
    "#         criterion='gini',      # Use 'entropy' if you prefer information gain\n",
    "#         max_depth=None,           # Limit tree depth if required\n",
    "#         random_state=42,        # For reproducibility\n",
    "#     ),\n",
    "#     n_jobs=-1  # Parallelize the MultiOutputClassifier\n",
    "# )\n",
    "\n",
    "model = MultiOutputClassifier(\n",
    "    KNeighborsClassifier(\n",
    "        n_neighbors=5,      # Number of neighbors to use\n",
    "        weights='uniform',  # Use 'distance' for weighted neighbors\n",
    "        algorithm='auto',   # Auto-select the best algorithm\n",
    "        n_jobs=-1           # Use all processors for parallelization\n",
    "    )\n",
    ")\n",
    "\n",
    "# model = MultiOutputClassifier(\n",
    "#     RadiusNeighborsClassifier(\n",
    "#         radius=5.0,         # Increased radius to accommodate more neighbors\n",
    "#         weights='uniform',  # Use 'distance' for weighted neighbors\n",
    "#         algorithm='auto',   # Auto-select the best algorithm\n",
    "#         outlier_label=-1,   # Label for outliers (you can set any value, here it's -1)\n",
    "#         n_jobs=-1           # Use all processors for parallelization\n",
    "#     )\n",
    "# )\n",
    "\n",
    "print(\"Training Decision Tree model...\")\n",
    "model.fit(train_data, train_labels)\n",
    "print(f\"Model training complete in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "print(\"Generating predictions on test data...\")\n",
    "start_time = time.time()\n",
    "test_predictions = model.predict_proba(test_data)  # [18886, 527]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "486bd73a-6312-4847-8924-99ec5e46e07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting probabilities for the positive class (class 1)...\n",
      "Calculating evaluation metrics...\n",
      "Calculating Mean Average Precision (MAP)...\n",
      "MAP calculation complete: 0.2488\n",
      "F1-Score calculation complete: 0.4414\n",
      "Metrics calculated in 25.88 seconds.\n",
      "\n",
      "--- Evaluation Results ---\n",
      "Test MAP: 0.2488\n",
      "Test F1-Score: 0.4414\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert predictions to a NumPy array\n",
    "test_predictions = np.array(test_predictions)\n",
    "\n",
    "# Extract probabilities for the positive class (class 1)\n",
    "print(\"Extracting probabilities for the positive class (class 1)...\")\n",
    "test_predictions = test_predictions[:, :, 1]  # Shape becomes (527, 18886)\n",
    "\n",
    "# Transpose predictions to match the labels' shape (18886, 527)\n",
    "test_predictions = test_predictions.T  # Shape becomes (18886, 527)\n",
    "\n",
    "# Metrics\n",
    "print(\"Calculating evaluation metrics...\")\n",
    "start_time = time.time()\n",
    "test_map = calculate_map(test_labels, test_predictions)\n",
    "test_f1 = f1_score(test_labels, (test_predictions > 0.5).astype(float), average=\"micro\")\n",
    "print(f\"F1-Score calculation complete: {test_f1:.4f}\")\n",
    "print(f\"Metrics calculated in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Test MAP: {test_map:.4f}\")\n",
    "print(f\"Test F1-Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24ea5253-7355-4fd8-a6fb-21d2a3a83a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18886\n"
     ]
    }
   ],
   "source": [
    "print(len(test_predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48b1a9-1cc4-465e-8b69-b0e786b119af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
