{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92dc551a-92e9-4c76-b5c2-940332b54111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Subset\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, average_precision_score\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "print(\"imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50568fba-b746-4bc3-a2f0-e445caef3553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54672884/ipykernel_2342659/3225634229.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x_train = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/train/train_rep.pt\")\n",
      "/state/partition1/job-54672884/ipykernel_2342659/3225634229.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  y_train = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/train/labels.pt\")\n",
      "/state/partition1/job-54672884/ipykernel_2342659/3225634229.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x_test = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/test/test_rep.pt\")\n",
      "/state/partition1/job-54672884/ipykernel_2342659/3225634229.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  y_test = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/test/labels.pt\")\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/train/train_rep.pt\") \n",
    "y_train = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/train/labels.pt\")\n",
    "x_test = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/test/test_rep.pt\")\n",
    "y_test = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/test/labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd2538d-2a51-4c14-a349-8db6323d2a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20550, 496, 768])\n",
      "torch.Size([18886, 496, 768])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c24d15f-b877-4003-8160-5e41d06fe9ce",
   "metadata": {},
   "source": [
    "# Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e316f09a-057e-42d5-8532-6a38d8a6be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_split(x_data, y_data, random_state=42, fold_idx=0):\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    indices = np.arange(len(y_data))\n",
    "    splits = list(mskf.split(indices, y_data))\n",
    "    if fold_idx >= len(splits): raise ValueError(f\"fold_idx {fold_idx} out of range for {len(splits)} splits\")\n",
    "    return splits[fold_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf5cf19-9cb4-4200-8659-b59e3b793c4e",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d74bd43-f8bd-4c6d-951f-8cc09c326dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map(y_true, y_pred):\n",
    "    n_classes = y_true.shape[1]\n",
    "    average_precisions = []\n",
    "    for i in range(n_classes):\n",
    "        y_true_class = y_true[:, i]\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        ap = average_precision_score(y_true_class, y_pred_class)\n",
    "        average_precisions.append(ap)\n",
    "    average_precisions_sorted = sorted(average_precisions, reverse=True)\n",
    "    print(\"\\nTop 5 class-wise Average Precisions:\")\n",
    "    for i, ap in enumerate(average_precisions_sorted[:5]):\n",
    "        print(f\"Class {i+1}: {ap:.4f}\")\n",
    "    \n",
    "    return np.mean(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca357837-b742-4adc-b00b-7429b9d6be39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0b5cea2-6835-4235-9ea5-a8b8dbdb89ab",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c3e6a94-50d9-4ec4-973b-1332c042d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_model(model, optimizer, criterion, train_loader, val_loader, transform_type='mean', num_epochs=10):\n",
    "    print(f\"\\nTraining MLP with {transform_type} transformation\")\n",
    "    model = model.to(device)\n",
    "    scaler = torch.amp.GradScaler('cuda')  \n",
    "    best_map = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda'): \n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "            if torch.isnan(loss):\n",
    "                print(f\"NaN loss detected at epoch {epoch}\")\n",
    "                continue\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation \n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.float().to(device)\n",
    "                batch_y = batch_y.float().to(device)\n",
    "                \n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        val_predictions = np.array(val_predictions)\n",
    "        val_targets = np.array(val_targets)\n",
    "        val_map = calculate_map(val_targets, val_predictions)\n",
    "        val_f1 = f1_score(val_targets, (val_predictions > 0.5).astype(float), average='micro')\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "        print(f'Validation MAP: {val_map:.4f}')\n",
    "        print(f'Validation F1-Score: {val_f1:.4f}')\n",
    "        \n",
    "        if val_map > best_map:\n",
    "            best_map = val_map\n",
    "            torch.save(model.state_dict(), f'best_model_mlp_{transform_type}_firstlayer.pth')\n",
    "            print(\"New best model saved!\")\n",
    "        \n",
    "        print('-' * 50)\n",
    "    \n",
    "    return model, best_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca19a96-9fe1-469d-838f-6866263023cc",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc7bd4a-3479-486c-be0a-e9cc5c1d7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mlp_model(model, transform_type):\n",
    "    print(f\"\\nTesting model with {transform_type} transformation\")\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    test_dataset = TensorDataset(x_test.float(), y_test.float())\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    model_path = f'best_model_mlp_{transform_type}_firstlayer.pth'\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None, None\n",
    "    \n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    test_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            \n",
    "            outputs = model(batch_x)\n",
    "            test_predictions.extend(outputs.cpu().numpy())\n",
    "            test_targets.extend(batch_y.cpu().numpy())\n",
    "    \n",
    "    test_predictions = np.array(test_predictions)\n",
    "    test_targets = np.array(test_targets)\n",
    "    \n",
    "    test_map = calculate_map(test_targets, test_predictions)\n",
    "    test_f1 = f1_score(test_targets, (test_predictions > 0.5).astype(float), average=\"micro\")\n",
    "    \n",
    "    print(f\"Test MAP: {test_map:.4f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    return test_map, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb2561-14d1-4396-bc47-d6bbfbe2a906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d066abc-5743-42ea-aa9e-eb67c342615f",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f2c25da-c076-4f51-8153-44fb00f6d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(x_data, transform_type, n_components=1024):\n",
    "    if transform_type == 'mean':\n",
    "        return torch.mean(x_data, dim=1)\n",
    "    elif transform_type == 'max':\n",
    "        return torch.max(x_data, dim=1)[0]\n",
    "    elif transform_type == 'append':\n",
    "        from sklearn.decomposition import PCA\n",
    "        x_flat = x_data.view(x_data.size(0), -1)\n",
    "        x_numpy = x_flat.cpu().numpy() if x_flat.is_cuda else x_flat.numpy()\n",
    "        pca = PCA(n_components=n_components)\n",
    "        x_reduced = pca.fit_transform(x_numpy)\n",
    "        return torch.tensor(x_reduced, dtype=x_data.dtype, device=x_data.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75bc0756-845f-4d8b-ad3c-09ed93134b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithTransform(nn.Module):\n",
    "    def __init__(self, input_size=768, hidden_size=1024, num_classes=527, transform_type='mean', n_components=None):\n",
    "        super(MLPWithTransform, self).__init__()\n",
    "        self.transform_type = transform_type\n",
    "        # self.n_components = n_components\n",
    "        self.input_size = input_size\n",
    "\n",
    "        if transform_type == 'append':\n",
    "            self.n_components = min(n_components or input_size, input_size)\n",
    "            self.register_buffer('pca_mean', None)\n",
    "            self.register_buffer('pca_components', None)\n",
    "            first_layer_input = self.n_components\n",
    "        else:\n",
    "            self.n_components = None\n",
    "            first_layer_input = input_size\n",
    "            \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(first_layer_input, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size // 2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.is_pca_fitted = False\n",
    "    \n",
    "    def load_pca(self, pca_path):\n",
    "        if self.transform_type == 'append':\n",
    "            try:\n",
    "                with open(pca_path, 'rb') as f:\n",
    "                    pca_data = pickle.load(f)\n",
    "                if not isinstance(pca_data, dict) or 'mean' not in pca_data or 'components' not in pca_data:\n",
    "                    raise ValueError(\"Invalid PCA file format\")\n",
    "                self.register_buffer('pca_mean', torch.from_numpy(pca_data['mean']).float())\n",
    "                self.register_buffer('pca_components', torch.from_numpy(pca_data['components']).float())\n",
    "                self.is_pca_fitted = True\n",
    "                print(\"PCA components loaded successfully!\")\n",
    "            except (EOFError, ValueError) as e:\n",
    "                print(f\"Error loading PCA file: {str(e)}\")\n",
    "                print(\"Will create new PCA file...\")\n",
    "                return False\n",
    "            # except Exception as e:\n",
    "            #     print(f\"Unexpected error loading PCA: {str(e)}\")\n",
    "            #     print(\"Will create new PCA file...\")\n",
    "            #     return False\n",
    "            return True\n",
    "    \n",
    "    def fit_pca(self, x, save_path=None):\n",
    "        if self.transform_type == 'append':\n",
    "            with torch.no_grad():\n",
    "                # x_flat = x.view(x.size(0), -1) \n",
    "                # x_numpy = x_flat.cpu().numpy() if x_flat.is_cuda else x_flat.numpy()\n",
    "                # First flatten the sequence dimension into the batch dimension\n",
    "                batch_size, seq_len, feat_dim = x.size()\n",
    "                x_reshaped = x.reshape(-1, feat_dim)  # Combine batch and sequence dims\n",
    "                # Now perform PCA\n",
    "                x_numpy = x_reshaped.cpu().numpy() if x_reshaped.is_cuda else x_reshaped.numpy()\n",
    "                pca = PCA(n_components=self.n_components)\n",
    "                pca.fit(x_numpy)\n",
    "                \n",
    "                self.register_buffer('pca_mean', torch.from_numpy(pca.mean_).float())\n",
    "                self.register_buffer('pca_components', torch.from_numpy(pca.components_).float())\n",
    "                self.is_pca_fitted = True\n",
    "                \n",
    "                if save_path:\n",
    "                    pca_data = {\n",
    "                        'mean': pca.mean_,\n",
    "                        'components': pca.components_\n",
    "                    }\n",
    "                    with open(save_path, 'wb') as f:\n",
    "                        pickle.dump(pca_data, f)\n",
    "                    print(f\"PCA components saved to {save_path}\")\n",
    "    \n",
    "    def transform_sequence(self, x):\n",
    "        if self.transform_type == 'mean':\n",
    "            return torch.mean(x, dim=1)\n",
    "        elif self.transform_type == 'max':\n",
    "            return torch.max(x, dim=1)[0]\n",
    "        elif self.transform_type == 'append':\n",
    "            if not self.is_pca_fitted:\n",
    "                raise RuntimeError(\"PCA must be fitted before transform. Call fit_pca first.\")\n",
    "            batch_size, seq_len, feat_dim = x.size()\n",
    "            x_reshaped = x.reshape(-1, feat_dim)  # [batch_size * seq_len, feat_dim]\n",
    "            x_centered = x_reshaped - self.pca_mean\n",
    "            x_transformed = torch.mm(x_centered, self.pca_components.t())\n",
    "            x_transformed = x_transformed.reshape(batch_size, seq_len, -1)\n",
    "            return torch.mean(x_transformed, dim=1)  # [batch_size, n_components]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown transform type: {self.transform_type}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transform_sequence(x)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a6e107-8b82-49aa-a1ee-9af804d1dc6e",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e82f9516-9f56-414d-a552-d3bdc5a38bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 20550\n",
      "Training samples: 16440\n",
      "Validation samples: 4110\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "x_train_float = x_train.float()\n",
    "y_train_float = y_train.float()\n",
    "full_dataset = TensorDataset(x_train_float, y_train_float)\n",
    "total_size = len(full_dataset)\n",
    "train_indices, val_indices = create_stratified_split(x_train_float, y_train_float.numpy())\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "print(f\"Total samples: {len(full_dataset)}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# print(\"x_train_float shape:\", x_train_float.shape)\n",
    "# print(\"x_train_float strides:\", x_train_float.stride())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True, pin_memory=torch.cuda.is_available())\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "transforms = [ 'mean', 'max', 'append' ] \n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e84e598-1f6b-4e3f-a776-d706e4e19f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mean transform...\n",
      "\n",
      "Training MLP with mean transformation\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.8837\n",
      "Class 2: 0.8723\n",
      "Class 3: 0.8411\n",
      "Class 4: 0.7843\n",
      "Class 5: 0.7817\n",
      "\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.2099\n",
      "Validation Loss: 0.0177\n",
      "Validation MAP: 0.1698\n",
      "Validation F1-Score: 0.2625\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.9924\n",
      "Class 2: 0.9552\n",
      "Class 3: 0.9507\n",
      "Class 4: 0.8979\n",
      "Class 5: 0.8609\n",
      "\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.0164\n",
      "Validation Loss: 0.0140\n",
      "Validation MAP: 0.3263\n",
      "Validation F1-Score: 0.3369\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9396\n",
      "Class 3: 0.9393\n",
      "Class 4: 0.9278\n",
      "Class 5: 0.9233\n",
      "\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.0140\n",
      "Validation Loss: 0.0127\n",
      "Validation MAP: 0.3961\n",
      "Validation F1-Score: 0.4036\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9809\n",
      "Class 3: 0.9679\n",
      "Class 4: 0.9654\n",
      "Class 5: 0.9566\n",
      "\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.0128\n",
      "Validation Loss: 0.0121\n",
      "Validation MAP: 0.4206\n",
      "Validation F1-Score: 0.4407\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 1.0000\n",
      "Class 3: 0.9663\n",
      "Class 4: 0.9650\n",
      "Class 5: 0.9565\n",
      "\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.0114\n",
      "Validation Loss: 0.0118\n",
      "Validation MAP: 0.4380\n",
      "Validation F1-Score: 0.4547\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9833\n",
      "Class 3: 0.9833\n",
      "Class 4: 0.9669\n",
      "Class 5: 0.9491\n",
      "\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.0110\n",
      "Validation Loss: 0.0117\n",
      "Validation MAP: 0.4436\n",
      "Validation F1-Score: 0.4637\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9812\n",
      "Class 3: 0.9791\n",
      "Class 4: 0.9783\n",
      "Class 5: 0.9617\n",
      "\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.0106\n",
      "Validation Loss: 0.0117\n",
      "Validation MAP: 0.4497\n",
      "Validation F1-Score: 0.4689\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9936\n",
      "Class 3: 0.9812\n",
      "Class 4: 0.9805\n",
      "Class 5: 0.9690\n",
      "\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.0102\n",
      "Validation Loss: 0.0116\n",
      "Validation MAP: 0.4496\n",
      "Validation F1-Score: 0.4898\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 1.0000\n",
      "Class 3: 0.9881\n",
      "Class 4: 0.9812\n",
      "Class 5: 0.9752\n",
      "\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.0095\n",
      "Validation Loss: 0.0115\n",
      "Validation MAP: 0.4543\n",
      "Validation F1-Score: 0.4920\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9881\n",
      "Class 3: 0.9764\n",
      "Class 4: 0.9758\n",
      "Class 5: 0.9727\n",
      "\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.0093\n",
      "Validation Loss: 0.0116\n",
      "Validation MAP: 0.4568\n",
      "Validation F1-Score: 0.4919\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing max transform...\n",
      "\n",
      "Training MLP with max transformation\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.7512\n",
      "Class 2: 0.6758\n",
      "Class 3: 0.5728\n",
      "Class 4: 0.5648\n",
      "Class 5: 0.5640\n",
      "\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.1810\n",
      "Validation Loss: 0.0207\n",
      "Validation MAP: 0.0721\n",
      "Validation F1-Score: 0.1326\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.9188\n",
      "Class 2: 0.8913\n",
      "Class 3: 0.8411\n",
      "Class 4: 0.8330\n",
      "Class 5: 0.7996\n",
      "\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.0191\n",
      "Validation Loss: 0.0174\n",
      "Validation MAP: 0.1829\n",
      "Validation F1-Score: 0.1854\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.9108\n",
      "Class 2: 0.9079\n",
      "Class 3: 0.8705\n",
      "Class 4: 0.8532\n",
      "Class 5: 0.8461\n",
      "\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.0166\n",
      "Validation Loss: 0.0166\n",
      "Validation MAP: 0.2414\n",
      "Validation F1-Score: 0.2072\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.9758\n",
      "Class 2: 0.9016\n",
      "Class 3: 0.8897\n",
      "Class 4: 0.8861\n",
      "Class 5: 0.8555\n",
      "\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.0152\n",
      "Validation Loss: 0.0142\n",
      "Validation MAP: 0.3018\n",
      "Validation F1-Score: 0.3469\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.9860\n",
      "Class 2: 0.9115\n",
      "Class 3: 0.9103\n",
      "Class 4: 0.9029\n",
      "Class 5: 0.8468\n",
      "\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.0139\n",
      "Validation Loss: 0.0139\n",
      "Validation MAP: 0.3251\n",
      "Validation F1-Score: 0.3353\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.9842\n",
      "Class 2: 0.9272\n",
      "Class 3: 0.9095\n",
      "Class 4: 0.9049\n",
      "Class 5: 0.9031\n",
      "\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.0134\n",
      "Validation Loss: 0.0140\n",
      "Validation MAP: 0.3370\n",
      "Validation F1-Score: 0.3091\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9254\n",
      "Class 3: 0.9233\n",
      "Class 4: 0.9099\n",
      "Class 5: 0.9044\n",
      "\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.0132\n",
      "Validation Loss: 0.0137\n",
      "Validation MAP: 0.3455\n",
      "Validation F1-Score: 0.3665\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9361\n",
      "Class 3: 0.9294\n",
      "Class 4: 0.9287\n",
      "Class 5: 0.9272\n",
      "\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.0130\n",
      "Validation Loss: 0.0130\n",
      "Validation MAP: 0.3650\n",
      "Validation F1-Score: 0.3930\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.9924\n",
      "Class 2: 0.9565\n",
      "Class 3: 0.9468\n",
      "Class 4: 0.9362\n",
      "Class 5: 0.9361\n",
      "\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.0123\n",
      "Validation Loss: 0.0131\n",
      "Validation MAP: 0.3690\n",
      "Validation F1-Score: 0.4088\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9428\n",
      "Class 3: 0.9427\n",
      "Class 4: 0.9333\n",
      "Class 5: 0.9030\n",
      "\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.0121\n",
      "Validation Loss: 0.0130\n",
      "Validation MAP: 0.3717\n",
      "Validation F1-Score: 0.4031\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing append transform...\n",
      "Fitting new PCA...\n",
      "PCA components saved to pca_components.pkl\n",
      "\n",
      "Training MLP with append transformation\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.8950\n",
      "Class 2: 0.8813\n",
      "Class 3: 0.8368\n",
      "Class 4: 0.8199\n",
      "Class 5: 0.8123\n",
      "\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.2137\n",
      "Validation Loss: 0.0175\n",
      "Validation MAP: 0.1912\n",
      "Validation F1-Score: 0.2818\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.9860\n",
      "Class 2: 0.9646\n",
      "Class 3: 0.9556\n",
      "Class 4: 0.9413\n",
      "Class 5: 0.9401\n",
      "\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.0163\n",
      "Validation Loss: 0.0138\n",
      "Validation MAP: 0.3431\n",
      "Validation F1-Score: 0.3653\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9752\n",
      "Class 3: 0.9630\n",
      "Class 4: 0.9323\n",
      "Class 5: 0.9321\n",
      "\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.0136\n",
      "Validation Loss: 0.0127\n",
      "Validation MAP: 0.3961\n",
      "Validation F1-Score: 0.4187\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9707\n",
      "Class 3: 0.9658\n",
      "Class 4: 0.9506\n",
      "Class 5: 0.9375\n",
      "\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.0123\n",
      "Validation Loss: 0.0120\n",
      "Validation MAP: 0.4261\n",
      "Validation F1-Score: 0.4671\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9742\n",
      "Class 3: 0.9623\n",
      "Class 4: 0.9618\n",
      "Class 5: 0.9434\n",
      "\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.0107\n",
      "Validation Loss: 0.0118\n",
      "Validation MAP: 0.4422\n",
      "Validation F1-Score: 0.4679\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9721\n",
      "Class 3: 0.9709\n",
      "Class 4: 0.9643\n",
      "Class 5: 0.9548\n",
      "\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.0101\n",
      "Validation Loss: 0.0118\n",
      "Validation MAP: 0.4492\n",
      "Validation F1-Score: 0.4870\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9860\n",
      "Class 3: 0.9614\n",
      "Class 4: 0.9577\n",
      "Class 5: 0.9373\n",
      "\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.0096\n",
      "Validation Loss: 0.0118\n",
      "Validation MAP: 0.4502\n",
      "Validation F1-Score: 0.5001\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9924\n",
      "Class 3: 0.9658\n",
      "Class 4: 0.9619\n",
      "Class 5: 0.9602\n",
      "\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.0092\n",
      "Validation Loss: 0.0118\n",
      "Validation MAP: 0.4496\n",
      "Validation F1-Score: 0.5060\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9924\n",
      "Class 3: 0.9631\n",
      "Class 4: 0.9560\n",
      "Class 5: 0.9556\n",
      "\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.0083\n",
      "Validation Loss: 0.0118\n",
      "Validation MAP: 0.4534\n",
      "Validation F1-Score: 0.5050\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 1.0000\n",
      "Class 2: 0.9842\n",
      "Class 3: 0.9688\n",
      "Class 4: 0.9605\n",
      "Class 5: 0.9603\n",
      "\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.0079\n",
      "Validation Loss: 0.0119\n",
      "Validation MAP: 0.4544\n",
      "Validation F1-Score: 0.5107\n",
      "New best model saved!\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for transform in transforms:\n",
    "    print(f\"\\nProcessing {transform} transform...\")\n",
    "    \n",
    "    model = MLPWithTransform(transform_type=transform)\n",
    "    \n",
    "    if transform == 'append':\n",
    "        pca_path = 'pca_components.pkl'\n",
    "        if os.path.exists(pca_path):\n",
    "            print(\"Loading saved PCA components...\")\n",
    "            model.load_pca(pca_path)\n",
    "        else:\n",
    "            print(\"Fitting new PCA...\")\n",
    "            model.fit_pca(x_train_float, save_path=pca_path)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    model, best_map = train_mlp_model(model, optimizer, criterion, train_loader, val_loader, transform_type=transform, num_epochs=num_epochs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d942b1a3-0321-4c1f-a2f3-b75b3e31b7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model with mean transformation\n",
      "Successfully loaded model from best_model_mlp_mean_firstlayer.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54672884/ipykernel_2342659/2756250242.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.9086\n",
      "Class 2: 0.9017\n",
      "Class 3: 0.9009\n",
      "Class 4: 0.9006\n",
      "Class 5: 0.8838\n",
      "Test MAP: 0.3905\n",
      "Test F1-Score: 0.4605\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing model with max transformation\n",
      "Successfully loaded model from best_model_mlp_max_firstlayer.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54672884/ipykernel_2342659/2756250242.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.8724\n",
      "Class 2: 0.8703\n",
      "Class 3: 0.8488\n",
      "Class 4: 0.8414\n",
      "Class 5: 0.8194\n",
      "Test MAP: 0.3234\n",
      "Test F1-Score: 0.3822\n",
      "--------------------------------------------------\n",
      "PCA components loaded successfully!\n",
      "\n",
      "Testing model with append transformation\n",
      "Successfully loaded model from best_model_mlp_append_firstlayer.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54672884/ipykernel_2342659/2756250242.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.9122\n",
      "Class 2: 0.9058\n",
      "Class 3: 0.9042\n",
      "Class 4: 0.8999\n",
      "Class 5: 0.8959\n",
      "Test MAP: 0.3929\n",
      "Test F1-Score: 0.4836\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for transform in transforms:\n",
    "    test_model = MLPWithTransform(transform_type=transform)\n",
    "    \n",
    "    if transform == 'append':\n",
    "        pca_path = 'pca_components.pkl'\n",
    "        if os.path.exists(pca_path):\n",
    "            test_model.load_pca(pca_path)\n",
    "    \n",
    "    test_model = test_model.to(device)\n",
    "    test_map, test_f1 = test_mlp_model(test_model, transform_type=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea54d468-4484-4eda-90ec-bd825e221003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
