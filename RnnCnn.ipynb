{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d59315-aedf-4947-a3ae-b5f84aa22348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torchaudio\n",
    "import librosa\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "910da5a1-1be0-4894-a5a5-42b68450b545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54255963/ipykernel_3668556/1539898298.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x_train = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/train/train_rep.pt\")\n",
      "/state/partition1/job-54255963/ipykernel_3668556/1539898298.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  y_train = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/train/labels.pt\")\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/train/train_rep.pt\")\n",
    "y_train = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/train/labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "515963af-ae7d-4f2d-87a6-3bc94e85b403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([20550, 496, 768])\n",
      "Training labels shape: torch.Size([20550, 527])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "# input: [20550, 496, 768]\n",
    "# 20550 -> number of samples\n",
    "# 496 -> sequence length (sequence of timesteps)\n",
    "# 768 -> number of features\n",
    "# output: [20550, 527] -> 527 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44c962d9-ba99-4b1c-9032-bb11063483ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44078b6-a322-4be5-a7de-d36b96dc4757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_channels = 768  \n",
    "# output_size = 527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b6c3870-45b2-479a-9a4d-e1e4ea98f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_channels=768, num_classes=527):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        # First Convolutional Block\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        #size of flattened features\n",
    "        self.flatten_size = 64 * (496 // (2*2*2))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.flatten_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae8a08ba-e7d7-49b3-8ade-42342f098cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map(y_true, y_pred):\n",
    "    n_classes = y_true.shape[1]\n",
    "    average_precisions = []\n",
    "    for i in range(n_classes):\n",
    "        y_true_class = y_true[:, i]\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        ap = average_precision_score(y_true_class, y_pred_class)\n",
    "        average_precisions.append(ap)\n",
    "    average_precisions_sorted = sorted(average_precisions, reverse=True)\n",
    "    print(\"\\nTop 5 class-wise Average Precisions:\")\n",
    "    for i, ap in enumerate(average_precisions_sorted[:5]):\n",
    "        print(f\"Class {i+1}: {ap:.4f}\")\n",
    "    \n",
    "    return np.mean(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4629392c-6c81-4d43-b737-751ee139f577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f78030ab-8370-43d9-b833-05a7cee3ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    best_map = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.float().to(device)\n",
    "                batch_y = batch_y.float().to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        # lists -> numpy arrays for metric calculation\n",
    "        val_predictions = np.array(val_predictions)\n",
    "        val_targets = np.array(val_targets)\n",
    "        val_map = calculate_map(val_targets, val_predictions)\n",
    "        val_f1 = f1_score(val_targets, (val_predictions > 0.5).astype(float), average='micro')\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "        print(f'Validation MAP: {val_map:.4f}')\n",
    "        print(f'Validation F1-Score: {val_f1:.4f}')\n",
    "        \n",
    "        # Save best model based on MAP\n",
    "        if val_map > best_map:\n",
    "            best_map = val_map\n",
    "            torch.save(model.state_dict(), 'best_model_map.pth')\n",
    "            print(\"New best model saved!\")\n",
    "        \n",
    "        print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f714f15e-402b-4612-a191-a77f55a56a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 20550\n",
      "Validation samples: 0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "x_train_float = x_train.float()\n",
    "y_train_float = y_train.float()\n",
    "\n",
    "full_dataset = TensorDataset(x_train_float, y_train_float)\n",
    "total_size = len(full_dataset)\n",
    "train_size = (1 * total_size)\n",
    "val_size = total_size - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size],generator=torch.Generator().manual_seed(42))\n",
    "    \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8793f50a-be10-468e-aaa2-54ec738878e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.8848\n",
      "Class 2: 0.8754\n",
      "Class 3: 0.8561\n",
      "Class 4: 0.8360\n",
      "Class 5: 0.8247\n",
      "\n",
      "Epoch 1/5:\n",
      "Training Loss: 0.0259\n",
      "Validation Loss: 0.0166\n",
      "Validation MAP: 0.2020\n",
      "Validation F1-Score: 0.2993\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.9316\n",
      "Class 2: 0.9304\n",
      "Class 3: 0.9265\n",
      "Class 4: 0.9168\n",
      "Class 5: 0.9041\n",
      "\n",
      "Epoch 2/5:\n",
      "Training Loss: 0.0172\n",
      "Validation Loss: 0.0136\n",
      "Validation MAP: 0.3328\n",
      "Validation F1-Score: 0.3889\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.9907\n",
      "Class 2: 0.9846\n",
      "Class 3: 0.9658\n",
      "Class 4: 0.9537\n",
      "Class 5: 0.9499\n",
      "\n",
      "Epoch 3/5:\n",
      "Training Loss: 0.0152\n",
      "Validation Loss: 0.0123\n",
      "Validation MAP: 0.4040\n",
      "Validation F1-Score: 0.4272\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.9909\n",
      "Class 2: 0.9904\n",
      "Class 3: 0.9839\n",
      "Class 4: 0.9656\n",
      "Class 5: 0.9619\n",
      "\n",
      "Epoch 4/5:\n",
      "Training Loss: 0.0139\n",
      "Validation Loss: 0.0111\n",
      "Validation MAP: 0.4697\n",
      "Validation F1-Score: 0.4776\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.9883\n",
      "Class 2: 0.9856\n",
      "Class 3: 0.9826\n",
      "Class 4: 0.9782\n",
      "Class 5: 0.9765\n",
      "\n",
      "Epoch 5/5:\n",
      "Training Loss: 0.0130\n",
      "Validation Loss: 0.0103\n",
      "Validation MAP: 0.5192\n",
      "Validation F1-Score: 0.5093\n",
      "New best model saved!\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = CNNClassifier()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "train_model(model, train_loader, train_loader, criterion, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea8cfe3c-2f3d-422d-acc7-e9a7b84314fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54255963/ipykernel_3668556/4202541997.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x_test = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/test/test_rep.pt\")\n",
      "/state/partition1/job-54255963/ipykernel_3668556/4202541997.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  y_test = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/test/labels.pt\")\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/test/test_rep.pt\")\n",
    "y_test = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/test/labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55e54a0a-baf4-4610-a26f-849534679a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(x_test, y_test)  \n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6435668c-bed8-4426-9a16-2fa429bddf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54255963/ipykernel_3668556/2448321432.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model_map.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNNClassifier(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv1d(768, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=3968, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=527, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model_map.pth\"))\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b24bbc92-0779-4898-b852-d285382f18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "test_targets = []  \n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader: \n",
    "        batch_x = batch_x.float().to(device)\n",
    "        outputs = model(batch_x)\n",
    "        test_predictions.extend(outputs.cpu().numpy())\n",
    "        if y_test is not None:\n",
    "            test_targets.extend(batch_y.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d26a7d4e-a602-4efd-833d-bc23a39cd51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 class-wise Average Precisions:\n",
      "Class 1: 0.9180\n",
      "Class 2: 0.9158\n",
      "Class 3: 0.9060\n",
      "Class 4: 0.8748\n",
      "Class 5: 0.8452\n",
      "Test MAP: 0.3339\n",
      "Test F1-Score: 0.4109\n"
     ]
    }
   ],
   "source": [
    "test_predictions = np.array(test_predictions)\n",
    "test_targets = np.array(test_targets) \n",
    "\n",
    "test_map = calculate_map(test_targets, test_predictions)\n",
    "test_f1 = f1_score(test_targets, (test_predictions > 0.5).astype(float), average=\"micro\")\n",
    "\n",
    "print(f\"Test MAP: {test_map:.4f}\")\n",
    "print(f\"Test F1-Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c93c7-0ebb-4e85-97c3-b3f4381ff84f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
