{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92dc551a-92e9-4c76-b5c2-940332b54111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Subset\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, average_precision_score\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "print(\"imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50568fba-b746-4bc3-a2f0-e445caef3553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54568671/ipykernel_2786278/3713021574.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x_train = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/train/train_rep.pt\")\n",
      "/state/partition1/job-54568671/ipykernel_2786278/3713021574.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  y_train = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/train/labels.pt\")\n",
      "/state/partition1/job-54568671/ipykernel_2786278/3713021574.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x_test = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/test/test_rep.pt\")\n",
      "/state/partition1/job-54568671/ipykernel_2786278/3713021574.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  y_test = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/test/labels.pt\")\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/train/train_rep.pt\")\n",
    "y_train = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/train/labels.pt\")\n",
    "x_test = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/test/test_rep.pt\")\n",
    "y_test = torch.load(\"/scratch/gd2574/AudioSet-classification/Data/test/labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2538d-2a51-4c14-a349-8db6323d2a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c24d15f-b877-4003-8160-5e41d06fe9ce",
   "metadata": {},
   "source": [
    "# Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e316f09a-057e-42d5-8532-6a38d8a6be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_split(x_data, y_data, random_state=42, fold_idx=0):\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    indices = np.arange(len(y_data))\n",
    "    splits = list(mskf.split(indices, y_data))\n",
    "    if fold_idx >= len(splits): raise ValueError(f\"fold_idx {fold_idx} out of range for {len(splits)} splits\")\n",
    "    return splits[fold_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf5cf19-9cb4-4200-8659-b59e3b793c4e",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d74bd43-f8bd-4c6d-951f-8cc09c326dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map(y_true, y_pred):\n",
    "    n_classes = y_true.shape[1]\n",
    "    average_precisions = []\n",
    "    for i in range(n_classes):\n",
    "        y_true_class = y_true[:, i]\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        ap = average_precision_score(y_true_class, y_pred_class)\n",
    "        average_precisions.append(ap)\n",
    "    average_precisions_sorted = sorted(average_precisions, reverse=True)\n",
    "    # print(\"\\nTop 5 class-wise Average Precisions:\")\n",
    "    # for i, ap in enumerate(average_precisions_sorted[:5]):\n",
    "    #     print(f\"Class {i+1}: {ap:.4f}\")\n",
    "    \n",
    "    return np.mean(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca357837-b742-4adc-b00b-7429b9d6be39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0b5cea2-6835-4235-9ea5-a8b8dbdb89ab",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c3e6a94-50d9-4ec4-973b-1332c042d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_model(model, optimizer, criterion, train_loader, val_loader, transform_type='mean', num_epochs=10):\n",
    "    print(f\"\\nTraining MLP with {transform_type} transformation\")\n",
    "    model = model.to(device)\n",
    "    scaler = torch.amp.GradScaler('cuda')  \n",
    "    best_map = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda'):  # Updated from torch.cuda.amp.autocast()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "            if torch.isnan(loss):\n",
    "                print(f\"NaN loss detected at epoch {epoch}\")\n",
    "                continue\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation \n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.float().to(device)\n",
    "                batch_y = batch_y.float().to(device)\n",
    "                \n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        val_predictions = np.array(val_predictions)\n",
    "        val_targets = np.array(val_targets)\n",
    "        val_map = calculate_map(val_targets, val_predictions)\n",
    "        val_f1 = f1_score(val_targets, (val_predictions > 0.5).astype(float), average='micro')\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "        print(f'Validation MAP: {val_map:.4f}')\n",
    "        print(f'Validation F1-Score: {val_f1:.4f}')\n",
    "        \n",
    "        if val_map > best_map:\n",
    "            best_map = val_map\n",
    "            torch.save(model.state_dict(), f'best_model_mlp_{transform_type}.pth')\n",
    "            print(\"New best model saved!\")\n",
    "        \n",
    "        print('-' * 50)\n",
    "    \n",
    "    return model, best_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca19a96-9fe1-469d-838f-6866263023cc",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fc7bd4a-3479-486c-be0a-e9cc5c1d7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mlp_model(model, transform_type):\n",
    "    print(f\"\\nTesting model with {transform_type} transformation\")\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    test_dataset = TensorDataset(x_test.float(), y_test.float())\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    model_path = f'best_model_mlp_{transform_type}.pth'\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None, None\n",
    "    \n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    test_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            \n",
    "            outputs = model(batch_x)\n",
    "            test_predictions.extend(outputs.cpu().numpy())\n",
    "            test_targets.extend(batch_y.cpu().numpy())\n",
    "    \n",
    "    test_predictions = np.array(test_predictions)\n",
    "    test_targets = np.array(test_targets)\n",
    "    \n",
    "    test_map = calculate_map(test_targets, test_predictions)\n",
    "    test_f1 = f1_score(test_targets, (test_predictions > 0.5).astype(float), average=\"micro\")\n",
    "    \n",
    "    print(f\"Test MAP: {test_map:.4f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    return test_map, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb2561-14d1-4396-bc47-d6bbfbe2a906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d066abc-5743-42ea-aa9e-eb67c342615f",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f2c25da-c076-4f51-8153-44fb00f6d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(x_data, transform_type, n_components=1024):\n",
    "    if transform_type == 'mean':\n",
    "        return torch.mean(x_data, dim=1)\n",
    "    elif transform_type == 'max':\n",
    "        return torch.max(x_data, dim=1)[0]\n",
    "    elif transform_type == 'append':\n",
    "        # Do PCA once\n",
    "        from sklearn.decomposition import PCA\n",
    "        x_flat = x_data.view(x_data.size(0), -1)\n",
    "        x_numpy = x_flat.cpu().numpy() if x_flat.is_cuda else x_flat.numpy()\n",
    "        pca = PCA(n_components=n_components)\n",
    "        x_reduced = pca.fit_transform(x_numpy)\n",
    "        return torch.tensor(x_reduced, dtype=x_data.dtype, device=x_data.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75bc0756-845f-4d8b-ad3c-09ed93134b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithTransform(nn.Module):\n",
    "    def __init__(self, input_size=768, hidden_size=1024, num_classes=527, transform_type='mean', n_components=1024):\n",
    "        super(MLPWithTransform, self).__init__()\n",
    "        self.transform_type = transform_type\n",
    "        self.n_components = n_components\n",
    "        self.input_size = input_size\n",
    "\n",
    "        if transform_type == 'append':\n",
    "            self.register_buffer('pca_mean', None)\n",
    "            self.register_buffer('pca_components', None)\n",
    "            first_layer_input = n_components\n",
    "        else:\n",
    "            first_layer_input = input_size\n",
    "            \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(first_layer_input, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size // 2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.is_pca_fitted = False\n",
    "    \n",
    "    def load_pca(self, pca_path):\n",
    "        if self.transform_type == 'append':\n",
    "            try:\n",
    "                with open(pca_path, 'rb') as f:\n",
    "                    pca_data = pickle.load(f)\n",
    "                if not isinstance(pca_data, dict) or 'mean' not in pca_data or 'components' not in pca_data:\n",
    "                    raise ValueError(\"Invalid PCA file format\")\n",
    "                self.register_buffer('pca_mean', torch.from_numpy(pca_data['mean']).float())\n",
    "                self.register_buffer('pca_components', torch.from_numpy(pca_data['components']).float())\n",
    "                self.is_pca_fitted = True\n",
    "                print(\"PCA components loaded successfully!\")\n",
    "            except (EOFError, ValueError) as e:\n",
    "                print(f\"Error loading PCA file: {str(e)}\")\n",
    "                print(\"Will create new PCA file...\")\n",
    "                return False\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error loading PCA: {str(e)}\")\n",
    "                print(\"Will create new PCA file...\")\n",
    "                return False\n",
    "            return True\n",
    "    \n",
    "    def fit_pca(self, x, save_path=None):\n",
    "        if self.transform_type == 'append':\n",
    "            with torch.no_grad():\n",
    "                x_flat = x.view(x.size(0), -1)\n",
    "                x_numpy = x_flat.cpu().numpy() if x_flat.is_cuda else x_flat.numpy()\n",
    "                pca = PCA(n_components=self.n_components)\n",
    "                pca.fit(x_numpy)\n",
    "                \n",
    "                self.register_buffer('pca_mean', torch.from_numpy(pca.mean_).float())\n",
    "                self.register_buffer('pca_components', torch.from_numpy(pca.components_).float())\n",
    "                self.is_pca_fitted = True\n",
    "                \n",
    "                if save_path:\n",
    "                    pca_data = {\n",
    "                        'mean': pca.mean_,\n",
    "                        'components': pca.components_\n",
    "                    }\n",
    "                    with open(save_path, 'wb') as f:\n",
    "                        pickle.dump(pca_data, f)\n",
    "                    print(f\"PCA components saved to {save_path}\")\n",
    "    \n",
    "    def transform_sequence(self, x):\n",
    "        if self.transform_type == 'mean':\n",
    "            return torch.mean(x, dim=1)\n",
    "        elif self.transform_type == 'max':\n",
    "            return torch.max(x, dim=1)[0]\n",
    "        elif self.transform_type == 'append':\n",
    "            if not self.is_pca_fitted:\n",
    "                raise RuntimeError(\"PCA must be fitted before transform. Call fit_pca first.\")\n",
    "            x_flat = x.view(x.size(0), -1)\n",
    "            x_centered = x_flat - self.pca_mean\n",
    "            x_transformed = torch.mm(x_centered, self.pca_components.t())\n",
    "            return x_transformed\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown transform type: {self.transform_type}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transform_sequence(x)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4abd375-9e16-4ddf-9558-5ab360f314ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64a6e107-8b82-49aa-a1ee-9af804d1dc6e",
   "metadata": {},
   "source": [
    "# lessgooo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e82f9516-9f56-414d-a552-d3bdc5a38bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 20550\n",
      "Training samples: 16440\n",
      "Validation samples: 4110\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "x_train_float = x_train.float()\n",
    "y_train_float = y_train.float()\n",
    "full_dataset = TensorDataset(x_train_float, y_train_float)\n",
    "total_size = len(full_dataset)\n",
    "train_indices, val_indices = create_stratified_split(x_train_float, y_train_float.numpy())\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "print(f\"Total samples: {len(full_dataset)}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True, pin_memory=torch.cuda.is_available())\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "transforms = [ 'mean', 'max','append'] #'mean', 'max',\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e84e598-1f6b-4e3f-a776-d706e4e19f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing mean transform...\n",
      "\n",
      "Training MLP with mean transformation\n",
      "\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.2088\n",
      "Validation Loss: 0.0176\n",
      "Validation MAP: 0.1673\n",
      "Validation F1-Score: 0.2581\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.0166\n",
      "Validation Loss: 0.0139\n",
      "Validation MAP: 0.3281\n",
      "Validation F1-Score: 0.3634\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.0140\n",
      "Validation Loss: 0.0128\n",
      "Validation MAP: 0.3930\n",
      "Validation F1-Score: 0.3987\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.0128\n",
      "Validation Loss: 0.0121\n",
      "Validation MAP: 0.4181\n",
      "Validation F1-Score: 0.4325\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.0114\n",
      "Validation Loss: 0.0118\n",
      "Validation MAP: 0.4391\n",
      "Validation F1-Score: 0.4607\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.0109\n",
      "Validation Loss: 0.0118\n",
      "Validation MAP: 0.4425\n",
      "Validation F1-Score: 0.4743\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.0106\n",
      "Validation Loss: 0.0117\n",
      "Validation MAP: 0.4488\n",
      "Validation F1-Score: 0.4689\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.0102\n",
      "Validation Loss: 0.0115\n",
      "Validation MAP: 0.4540\n",
      "Validation F1-Score: 0.4907\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.0095\n",
      "Validation Loss: 0.0115\n",
      "Validation MAP: 0.4575\n",
      "Validation F1-Score: 0.4986\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.0092\n",
      "Validation Loss: 0.0115\n",
      "Validation MAP: 0.4575\n",
      "Validation F1-Score: 0.4993\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing max transform...\n",
      "\n",
      "Training MLP with max transformation\n",
      "\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.1867\n",
      "Validation Loss: 0.0208\n",
      "Validation MAP: 0.0755\n",
      "Validation F1-Score: 0.1287\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.0189\n",
      "Validation Loss: 0.0186\n",
      "Validation MAP: 0.1701\n",
      "Validation F1-Score: 0.2433\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.0164\n",
      "Validation Loss: 0.0157\n",
      "Validation MAP: 0.2535\n",
      "Validation F1-Score: 0.2956\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.0151\n",
      "Validation Loss: 0.0142\n",
      "Validation MAP: 0.3059\n",
      "Validation F1-Score: 0.3188\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.0138\n",
      "Validation Loss: 0.0143\n",
      "Validation MAP: 0.3228\n",
      "Validation F1-Score: 0.3263\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.0134\n",
      "Validation Loss: 0.0137\n",
      "Validation MAP: 0.3413\n",
      "Validation F1-Score: 0.3690\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.0131\n",
      "Validation Loss: 0.0137\n",
      "Validation MAP: 0.3414\n",
      "Validation F1-Score: 0.3752\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.0128\n",
      "Validation Loss: 0.0131\n",
      "Validation MAP: 0.3577\n",
      "Validation F1-Score: 0.4003\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.0122\n",
      "Validation Loss: 0.0129\n",
      "Validation MAP: 0.3692\n",
      "Validation F1-Score: 0.4109\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.0121\n",
      "Validation Loss: 0.0131\n",
      "Validation MAP: 0.3661\n",
      "Validation F1-Score: 0.4120\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing append transform...\n",
      "Loading saved PCA components...\n",
      "PCA components loaded successfully!\n",
      "\n",
      "Training MLP with append transformation\n",
      "\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.2494\n",
      "Validation Loss: 0.0189\n",
      "Validation MAP: 0.1350\n",
      "Validation F1-Score: 0.2587\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.0168\n",
      "Validation Loss: 0.0141\n",
      "Validation MAP: 0.3205\n",
      "Validation F1-Score: 0.3360\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.0135\n",
      "Validation Loss: 0.0131\n",
      "Validation MAP: 0.3764\n",
      "Validation F1-Score: 0.4107\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.0117\n",
      "Validation Loss: 0.0125\n",
      "Validation MAP: 0.4047\n",
      "Validation F1-Score: 0.4433\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.0098\n",
      "Validation Loss: 0.0124\n",
      "Validation MAP: 0.4170\n",
      "Validation F1-Score: 0.4638\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.0089\n",
      "Validation Loss: 0.0125\n",
      "Validation MAP: 0.4176\n",
      "Validation F1-Score: 0.4732\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.0083\n",
      "Validation Loss: 0.0127\n",
      "Validation MAP: 0.4192\n",
      "Validation F1-Score: 0.4751\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.0077\n",
      "Validation Loss: 0.0127\n",
      "Validation MAP: 0.4209\n",
      "Validation F1-Score: 0.4879\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.0068\n",
      "Validation Loss: 0.0130\n",
      "Validation MAP: 0.4213\n",
      "Validation F1-Score: 0.4933\n",
      "New best model saved!\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.0063\n",
      "Validation Loss: 0.0131\n",
      "Validation MAP: 0.4218\n",
      "Validation F1-Score: 0.4935\n",
      "New best model saved!\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for transform in transforms:\n",
    "    print(f\"\\nProcessing {transform} transform...\")\n",
    "    \n",
    "    model = MLPWithTransform(transform_type=transform)\n",
    "    \n",
    "    if transform == 'append':\n",
    "        pca_path = 'pca_components.pkl'\n",
    "        if os.path.exists(pca_path):\n",
    "            print(\"Loading saved PCA components...\")\n",
    "            model.load_pca(pca_path)\n",
    "        else:\n",
    "            print(\"Fitting new PCA...\")\n",
    "            model.fit_pca(x_train_float, save_path=pca_path)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    model, best_map = train_mlp_model(model, optimizer, criterion, train_loader, val_loader, transform_type=transform, num_epochs=num_epochs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d942b1a3-0321-4c1f-a2f3-b75b3e31b7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model with mean transformation\n",
      "Successfully loaded model from best_model_mlp_mean.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54568671/ipykernel_2786278/3450733841.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAP: 0.3912\n",
      "Test F1-Score: 0.4718\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing model with max transformation\n",
      "Successfully loaded model from best_model_mlp_max.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54568671/ipykernel_2786278/3450733841.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAP: 0.3186\n",
      "Test F1-Score: 0.3851\n",
      "--------------------------------------------------\n",
      "PCA components loaded successfully!\n",
      "\n",
      "Testing model with append transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54568671/ipykernel_2786278/3450733841.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model from best_model_mlp_append.pth\n",
      "Test MAP: 0.3670\n",
      "Test F1-Score: 0.4732\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing for all transforms\n",
    "for transform in transforms:\n",
    "    # Create a new model instance for each transform\n",
    "    test_model = MLPWithTransform(transform_type=transform)\n",
    "    \n",
    "    # For append transform, load PCA if it exists\n",
    "    if transform == 'append':\n",
    "        pca_path = 'pca_components.pkl'\n",
    "        if os.path.exists(pca_path):\n",
    "            test_model.load_pca(pca_path)\n",
    "    \n",
    "    # Move model to device\n",
    "    test_model = test_model.to(device)\n",
    "    \n",
    "    # Test the model\n",
    "    test_map, test_f1 = test_mlp_model(test_model, transform_type=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea54d468-4484-4eda-90ec-bd825e221003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
